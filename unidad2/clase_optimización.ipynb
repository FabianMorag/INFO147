{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización\n",
    "\n",
    "Es el proceso para:\n",
    "- **(Prosa)** encontrar la mejor solución de un problema\n",
    "- **(Matematicamente)** encontrar un valor extremo de una función\n",
    "\n",
    "### Definiciones\n",
    "\n",
    "- Función objetivo: Función continua que queremos optimizar, $f : \\mathbb{R}^D \\to \\mathbb{R}$\n",
    "    - Puede ser diferenciable\n",
    "    - Puede ser convexa\n",
    "- Valor extremo: Mínimo o máximo de la función objetivo, $ \\max f(\\vec x) \\equiv \\min - f(\\vec x)$\n",
    "    - Es suficiente hablar de minimización\n",
    "- Un mínimo $x^*$ es tal que $f(x) > f(x^*)$ para $x \\in \\mathbb{S}$\n",
    "    - Si $\\mathbb{S}$ es igual al dominio de $f(x)$ entonces es un mínimo global\n",
    "    - De lo contrario hablamos de un mínimo local\n",
    "    - Una función convexa tiene sólo un mínimo\n",
    "- La solución que buscamos podría estar sujeta a restricciones\n",
    "\n",
    "\n",
    "### Problema general de optimización\n",
    "\n",
    "Para una función $f : \\mathbb{R}^D \\to \\mathbb{R}$\n",
    "$$\n",
    "\\min_x f(x) ~ \\text{s.a.} ~g(x) = 0, h(x) \\leq 0,\n",
    "$$\n",
    "donde $g : \\mathbb{R}^D \\to \\mathbb{R}^G$ y $h : \\mathbb{R}^D \\to \\mathbb{R}^H$\n",
    "\n",
    "#### Clasificación de problemas de optimización\n",
    "\n",
    "- Una variable versus multi-variable\n",
    "- Ecuaciones lineales o no-lineales (convexo o no convexo)\n",
    "- Sin/con restricciones (sin/con desigualdades)\n",
    "\n",
    "Mínimos cuadrados: Multi-variable, lineal, sin restricciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo encontrar el mínimo de una función continua sin restricciones?\n",
    "\n",
    "Encontrar las raices (ceros) de la derivada/gradiente de $f$\n",
    "\n",
    "$$\n",
    "\\nabla f (\\theta^*) = \\begin{pmatrix} \\frac{\\partial f}{\\partial \\theta_1}, \\frac{\\partial f}{\\partial \\theta_2}, \\ldots, \\frac{\\partial f}{\\partial \\theta_D} \\end{pmatrix} = \\vec 0\n",
    "$$\n",
    "\n",
    "Las soluciones se conocen como puntos estacionarios de $f$\n",
    "\n",
    "Luego si las segunda derivada/Hessiano de $f$\n",
    "\n",
    "$$\n",
    "H_{ij}^f (\\theta^*)  = \\frac{\\partial^2 f}{\\partial \\theta_i \\partial \\theta_j} (\\theta^*)\n",
    "$$\n",
    "\n",
    "es positiva/semi-definida positiva  entonces $\\theta^*$ es un **mínimo local**\n",
    "\n",
    "\n",
    "Receta\n",
    "1. Obtener $\\theta^*$ tal que $\\nabla f (\\theta^*)=0$\n",
    "1. Probar que es un mínimo el Hessiano\n",
    "\n",
    "##### Problema: Sólo sirve si podemos obtener una expresión análitica de $\\theta$ a partir de $\\nabla f (\\theta^*)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Regresión con elemento no-lineal\n",
    "\n",
    "- Sea un conjunto de $M=100$ datos que corresponden a dos categorías, $y_i \\in \\{0, 1\\}$\n",
    "- Los datos son bidimensionales, $x_i \\in \\mathbb{R}^2$\n",
    "- Para mapear los datos a la categoría pasar un **hiperplano** a través de  una **función no-lineal**\n",
    "\n",
    "$$\n",
    "\\sigma \\left ( \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} \\right) \\approx y_i ~ \\forall i=1,\\ldots,100\n",
    "$$\n",
    "por ejemplo\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-x}} \\in [0, 1],\n",
    "$$\n",
    "que se conoce como función sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "z = np.linspace(-6, 6, num=100)\n",
    "ax.plot(z, 1/(1+np.exp(-z)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos intentar ajustar \n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{i=1}^M \\left(y_i - \\sigma \\left ( \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} \\right)\\right)^2\n",
    "$$\n",
    "\n",
    "Pero: \n",
    "- Ya no es lineal en sus parámetros\n",
    "- No podremos despejar en función de $\\theta$\n",
    "- No hay solución análitica como en mínimos cuadrados lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "X = np.concatenate((np.random.randn(N//2, 2), 2+np.random.randn(N//2, 2)), axis=0)\n",
    "Y = np.concatenate((np.zeros(shape=(N//2, 1)), np.ones(shape=(N//2, 1))), axis=0)[:, 0]\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c='b', label='perros')\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='r', label='gatos')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Búsqueda exhaustiva de la mejor solución\n",
    "\n",
    "También conocido como método de fuerza bruta\n",
    "1. Definimos una grilla para nuestro espacio de parámetros (dominio y resolución)\n",
    "1. Para cada elemento de la grilla calculamos la función de costo\n",
    "1. Buscamos el elemento que menor función de costo\n",
    "\n",
    "Ventaja: Si la resolución es lo suficientemente fina podemos encontrar el mínimo global del dominio\n",
    "\n",
    "Desventaja: Costo computacional, explosión combinatorial\n",
    "\n",
    "\n",
    "Sea un modelo de $10$ parámetros con una resolución de $1000$ puntos cada uno: $1000^{10}$ evaluaciones de $f()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def f(theta0, theta1, theta2):\n",
    "    model = theta0 + theta1*X[:, 0] + theta2*X[:, 1]\n",
    "    sig = 1./(1. + np.exp(-model))\n",
    "    return np.mean((Y-sig)**2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "theta0 = -5.4\n",
    "theta1 = np.linspace(-10, 10, num=1000)\n",
    "theta2 = np.linspace(-10, 10, num=1000)\n",
    "T1, T2 = np.meshgrid(theta1, theta2)\n",
    "mse_plot = np.zeros(shape=(len(theta1), len(theta2)))\n",
    "for i, t1_ in enumerate(theta1):\n",
    "    for j, t2_ in enumerate(theta2):\n",
    "        mse_plot[i, j] = f(theta0, t1_, t2_)\n",
    "cf = ax.contourf(T1, T2, mse_plot)\n",
    "plt.colorbar(cf)\n",
    "idx = np.unravel_index(np.argmin(mse_plot), mse_plot.shape)\n",
    "ax.scatter(theta1[idx[1]], theta2[idx[0]], s=100, c='r');\n",
    "display(theta0, theta1[idx[1]], theta2[idx[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método iterativos: Newton\n",
    "\n",
    "\n",
    "- Acercarse a la mejor solución paso a paso\n",
    "- Encontrar en cada instante la mejor dirección\n",
    "\n",
    "Sea el valor actual del vector de parámetros $\\theta_t$\n",
    "\n",
    "Queremos encontrar el mejor \"próximo valor\" según nuestra función objetivo\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\Delta \\theta\n",
    "$$\n",
    "Consideremos la aproximación de Taylor de segundo orden de $f$\n",
    "$$\n",
    "f(\\theta_{t} + \\Delta \\theta) \\approx f(\\theta_t) + \\nabla f (\\theta_t) \\Delta \\theta + \\frac{1}{2} \\Delta \\theta^T H_f (\\theta_t) \\Delta \\theta \n",
    "$$\n",
    "Derivando en función de $\\Delta \\theta$ e igualando a cero tenemos\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla f (\\theta_t)  +  H_f (\\theta_t) \\Delta \\theta &= 0 \\nonumber \\\\\n",
    "\\Delta \\theta &= - [H_f (\\theta_t)]^{-1}\\nabla f (\\theta_t)  \\nonumber \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} - [H_f (\\theta_t)]^{-1}\\nabla f (\\theta_t)  \\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Se obtiene una regla iterativa en función del **Gradiente** y del **Hessiano**\n",
    "- La solución depende de $\\theta_0$\n",
    "- \"Asumimos\" que la aproximación de segundo orden es \"buena\"\n",
    "- Si nuestro modelo tiene $N$ parámetros el Hessiano es de $N\\times N$, ¿Qué pasa si $N$ es grande?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente descendente\n",
    "\n",
    "Si el Hessiano es prohibitivo podemos usar una aproximación de primer orden\n",
    "\n",
    "El método más clásico es el **gradiente descendente**\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla f (\\theta_t)\n",
    "$$\n",
    "\n",
    "donde hemos reemplazado el Hessiano por una constante $\\eta$ llamado \"paso\" o \"tasa de aprendizaje\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "plt.close('all'); fig, ax = plt.subplots(2, figsize=(7, 4), tight_layout=True, sharex=True)\n",
    "x = np.linspace(-4, 6, num=100)\n",
    "f = lambda theta : (theta-1.)**2 #+ 10*np.sin(theta)\n",
    "df = lambda theta : 2*(theta -1.) #+ 10*np.cos(theta)\n",
    "df2 = lambda theta : 2 #- 10*np.cos(theta)\n",
    "\n",
    "t = 10*np.random.rand(10) - 4.\n",
    "ax[0].plot(x, f(x))\n",
    "sc = ax[0].scatter(t, L(t), s=100)\n",
    "\n",
    "ax[1].set_xlabel(r'$\\theta$')\n",
    "ax[0].set_ylabel(r'$f(\\theta)$')\n",
    "ax[1].plot(x, -df(x))\n",
    "ax[1].set_ylabel(r'- f(\\theta)$')\n",
    "eta = 0.01\n",
    "\n",
    "def update(n):\n",
    "    t = sc.get_offsets()[:, 0]\n",
    "    t -= eta*df(t)\n",
    "    #t -= df(t)/(df2(t)+10)\n",
    "    sc.set_offsets(np.c_[t, f(t)])\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update, frames=100, interval=200, repeat=False, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con elemento no lineal usando gradiente descendente\n",
    "\n",
    "- Calcule el gradiente de la función de costo en función de los parámetros\n",
    "- Pruebe distintos valores de $\\eta$ y estudie lo que ocurre con la solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta0, theta1, theta2 = np.random.randn(3).T\n",
    "sigmoid = lambda z: 1./(1+np.exp(-z))\n",
    "eta = 2e-2\n",
    "mse_plot = np.zeros(shape=(500,))\n",
    "for t in range(len(mse_plot)):\n",
    "    model = theta0 + theta1*X[:, 0] + theta2*X[:, 1]\n",
    "    tmp = -2*(Y - sigmoid(model))*(1 - sigmoid(model))\n",
    "    theta0 -= eta*np.sum(tmp)\n",
    "    theta1 -= eta*np.sum(tmp*X[:, 0])\n",
    "    theta2 -= eta*np.sum(tmp*X[:, 1])\n",
    "    mse_plot[t] = np.sum((Y - sigmoid(model))**2)\n",
    "    if t % 50 == 0:\n",
    "        print(t, theta0, theta1, theta2, mse_plot[t])\n",
    "\n",
    "y_hat = sigmoid(theta0 + theta1*X[:, 0] + theta2*X[:, 1])\n",
    "\n",
    "fig, ax = plt.subplots(2, figsize=(7, 5), tight_layout=True)\n",
    "ax[1].scatter(X[Y==0, 0], X[Y==0, 1], c='k', marker='x', label='perros')\n",
    "ax[1].scatter(X[Y==1, 0], X[Y==1, 1], c='k', marker='o', label='gatos')\n",
    "x_lim, y_lim = ax[1].get_xlim(), ax[1].get_ylim()\n",
    "x = np.linspace(x_lim[0], x_lim[1], num=100)\n",
    "y = np.linspace(y_lim[0], y_lim[1], num=100)\n",
    "X_plot, Y_plot = np.meshgrid(x, y)\n",
    "C = sigmoid(theta0 + theta1*X_plot + theta2*Y_plot)\n",
    "cf = ax[1].contourf(X_plot, Y_plot, C, zorder=-100, cmap=plt.cm.RdBu_r)\n",
    "plt.colorbar(cf, ax=ax[1])\n",
    "plt.legend();\n",
    "ax[0].plot(mse_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulo [`scipy.optimize`](https://docs.scipy.org/doc/scipy-1.3.0/reference/tutorial/optimize.html)\n",
    "\n",
    "La función [`minimize`](https://docs.scipy.org/doc/scipy-1.3.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) engloba una batería de optimizadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "theta0, theta1, theta2 = np.random.randn(3).T\n",
    "sigmoid = lambda z: 1/(1+np.exp(-z))\n",
    "\n",
    "def f(theta):\n",
    "    model = theta[0] + theta[1]*X[:, 0] + theta[2]*X[:, 1]\n",
    "    return np.sum((Y - sigmoid(model))**2)\n",
    "\n",
    "theta0 = np.random.randn(3)\n",
    "res = scipy.optimize.minimize(f, theta0, method='BFGS', options={'disp': True})\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = res.x\n",
    "y_hat = sigmoid(theta[0] + theta[1]*X[:, 0] + theta[2]*X[:, 1])\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.scatter(X[Y==0, 0], X[Y==0, 1], c=y_hat[Y==0], \n",
    "           marker='x', label='perros', vmin=0, vmax=1, cmap=plt.cm.RdBu_r)\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c=y_hat[Y==1], \n",
    "           marker='o', label='gatos', vmin=0, vmax=1, cmap=plt.cm.RdBu_r)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apéndices\n",
    "\n",
    "https://www.cs.toronto.edu/~frossard/post/linear_regression/\n",
    "\n",
    "https://mmas.github.io/least-squares-fitting-numpy-scipy\n",
    "\n",
    "https://sbu-python-class.github.io/python-science/\n",
    "\n",
    "https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12\n",
    "\n",
    "http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n",
    "\n",
    "http://people.duke.edu/~ccc14/sta-663-2018/notebooks/S09D_Optimization_Examples.html\n",
    "\n",
    "http://learningwithdata.com/logistic-regression-and-optimization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
