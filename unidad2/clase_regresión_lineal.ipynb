{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "Consiste en **ajustar** un modelo paramétrico\n",
    "$$\n",
    "f_\\theta: x \\rightarrow y\n",
    "$$\n",
    "que sea capaz de predecir $y$ dado $x$\n",
    "\n",
    "- $x$ variable indepediente, entrada, característica, predictor\n",
    "- $y$ variable dependiente, salida, respuesta, objetivo (target)\n",
    "- $x$ e $y$ son variables continuas\n",
    "- $\\theta$ son los parámetros del modelo\n",
    "\n",
    "> Encontrar como dos o más variables se relacionan. Explicar una variable en función de otras. Predicción\n",
    "\n",
    "Hablamos de **regresión lineal** cuando el modelo $f_\\theta$ es **lineal en sus parámetros**\n",
    "\n",
    "¿Son estos modelos lineales en sus parámetros?\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f_\\theta(x) = \\theta_0  + \\theta_1 x  \\nonumber \\\\\n",
    "y &= f_\\theta(x) = \\theta_0  + \\theta_1 x + \\theta_2 x^2 + \\theta_3 \\log(x) \\nonumber \\\\\n",
    "y &= f_\\theta(x) = \\theta_0  + \\sin(\\theta_1) x  \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Datos:** conjunto de $M$ tuplas $(\\vec x_i, y_i)$ con $i=1,2,\\ldots,M$ \n",
    "- **Ajuste:** Encontrar el valor óptimo de $\\theta$ en función de los datos\n",
    "- Podemos escribir el problema como un **sistema lineal de $M$ ecuaciones**\n",
    "- Si el sistema es rectangular lo podemos resolver con **Mínimos Cuadrados**\n",
    "- En dicho caso la solución es óptima *\"en el sentido de mínimos cuadrádos\"*\n",
    "\n",
    "### Modelos lineales en sus parámetros y en sus entradas\n",
    "#### Recta\n",
    "Si $x$ es unidimensional \n",
    "$$\n",
    "y =\\theta_0  + \\theta_1 x \n",
    "$$\n",
    "\n",
    "#### Plano\n",
    "Si $\\vec x =(x_1, x_2)$ es bidimensional \n",
    "$$\n",
    "y = \\theta_0  + \\theta_1 x_1 +  \\theta_2 x_2 \n",
    "$$\n",
    "\n",
    "#### Hiperplano\n",
    "Si $\\vec x = (x_1, x_2, \\ldots, x_d)$ es d-dimensional\n",
    "$$\n",
    "y = \\theta_0  + \\sum_{k=1}^d \\theta_k x_k \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, SelectionSlider, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.close('all'); fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "theta = [4, 3, 2]; \n",
    "\n",
    "def update(rseed, N, sigma):\n",
    "    ax.cla();\n",
    "    np.random.seed(rseed);\n",
    "    x1, x2 = np.random.randn(2, N)\n",
    "    y_clean = theta[0] + theta[1]*x1 + theta[2]*x2 \n",
    "    y = y_clean + sigma*np.random.randn(len(x1))\n",
    "    X_lstsq = np.stack((np.ones_like(x1), x1, x2)).T\n",
    "    param, MSE, rank, singval = np.linalg.lstsq(X_lstsq, y, rcond=None)\n",
    "    display(theta, param)\n",
    "    ax.scatter(x1, x2, y, s=10, label='data')\n",
    "    X1, X2 = np.meshgrid(np.linspace(np.amin(x1), np.amax(x1), num=2), \n",
    "                         np.linspace(np.amin(x2), np.amax(x2), num=2))\n",
    "    ax.plot_surface(X1, X2, param[0] + param[1]*X1 + param[2]*X2, \n",
    "                    label='model', alpha=0.25)\n",
    "\n",
    "interact(update, \n",
    "         rseed=IntSlider(continuous_update=False), \n",
    "         N=SelectionSlider(options=[10, 100, 1000]),\n",
    "         sigma=SelectionSlider(options=[0.1, 0.5, 1, 2, 5, 10.]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción: Interpolación y Extrapolación\n",
    "\n",
    "Una vez que el regresor ha sido ajustado se puede usar para hacer predicciónes de la variable dependiente a partir de valores no observados de la variable independiente\n",
    "\n",
    "- Llamamos interpolación cuando predecimos dentro del rango de nuestros datos\n",
    "- Llamamos extrapolación cuando predecimos fuera del rango de nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(5)\n",
    "y = lambda x: -0.75*x**2 + 5*x -4\n",
    "theta = np.linalg.lstsq(np.stack((np.ones_like(x), x)).T, y(x), rcond=None)[0]\n",
    "y_hat = lambda x : np.dot(np.stack((np.ones_like(x), x)).T, theta)\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.scatter(x, y(x), c='k')\n",
    "ax.scatter(3, y(3), c='r')\n",
    "x_plot = np.linspace(np.amin(x), np.amax(x))\n",
    "ax.plot(x_plot, y_hat(x_plot))\n",
    "x_plot = np.linspace(np.amax(x), 3)\n",
    "ax.plot(x_plot, y_hat(x_plot))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos lineales en sus parámetros pero no en sus entradas\n",
    "\n",
    "Podemos generalizar la regresión lineal usando funciónes base $\\phi_j(\\cdot)$ tal que el modelo\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\sum_{j=0}^N \\theta_j \\phi_j (x)\n",
    "$$\n",
    "\n",
    "#### Regresión lineal con polinomios\n",
    "\n",
    "Si usamos $\\phi_j(x) = x^j$ nos queda\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\ldots\n",
    "$$\n",
    "\n",
    "#### Regresión lineal con sinusoides\n",
    "\n",
    "Si usamos $\\phi_j(x) = \\cos(2\\pi j x)$ nos queda\n",
    "\n",
    "$$\n",
    "y = f_\\theta (x) = \\theta_0 + \\theta_1 \\cos(2\\pi x) + \\theta_2 \\cos(4 \\pi x) + \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, num=100)\n",
    "y = 2*np.cos(2.0*np.pi*x) + np.sin(4.0*np.pi*x) + 0.4*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "ax.scatter(x, y)\n",
    "\n",
    "poly_basis = lambda x,N : np.vstack([x**k for k in range(N)]).T\n",
    "N = 1\n",
    "theta = np.linalg.lstsq(poly_basis(x, N), y, rcond=None)[0]\n",
    "ax.plot(x, np.dot(poly_basis(x, N), theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema infradeterminado\n",
    "\n",
    "Es aquel sistema que tiene más incognitas (parámetros) que ecuaciones, $N>M$\n",
    "\n",
    "Este tipo de sistema tiene infinitas soluciones\n",
    "\n",
    "#### Ejemplo: Dos puntos con polinomio de segundo orden (tres parámetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2, 2])\n",
    "y = np.array([4, 4])\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "x_plot = np.linspace(-3, 3, num=100)\n",
    "thetas = np.zeros(shape=(200, 3))\n",
    "for i, a in enumerate(np.linspace(-10, 10, num=thetas.shape[0])):\n",
    "    ax.plot(x_plot, a  + (1 - a/4)*x_plot**2)\n",
    "    thetas[i:] = [a, 0, (1-a/4)]\n",
    "ax.scatter(x, y, s=100, c='k', zorder=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso $A^T A$ no es invertible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = poly_basis(x, N=3)\n",
    "display(A)\n",
    "np.linalg.inv(np.dot(A.T, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema infradeterminado se resuelve imponiendo una restricción adicional\n",
    "\n",
    "La más típica es que el vector solución tenga norma mínima\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\| x \\|_2^2 ~\\text{s.a.}~ Ax =b\n",
    "$$\n",
    "\n",
    "que se resuelve usando $M$ multiplicadores de Lagrande\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} \\| x\\|_2^2 + \\lambda^T (b - Ax) &= 2x - \\lambda^T A  \\nonumber \\\\\n",
    "&= 2Ax - A A^T \\lambda \\nonumber \\\\\n",
    "&= 2b - A A^T \\lambda = 0 \\nonumber \\\\\n",
    "&\\rightarrow \\lambda = 2(AA^T)^{-1}b \\nonumber \\\\\n",
    "&\\rightarrow x = \\frac{1}{2} A^T \\lambda = A^T (A A^T)^{-1} b\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "donde $A^T (A A^T)^{-1}$ se conoce como la pseudo-inversa \"por la derecha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([-2, 2])\n",
    "y = np.array([4, 4])\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "x_plot = np.linspace(-3, 3, num=100)\n",
    "theta = np.dot(np.dot(A.T, np.linalg.inv(np.dot(A, A.T))), y)\n",
    "ax.plot(x_plot, np.dot(poly_basis(x_plot, N=3), theta))\n",
    "ax.scatter(x, y, s=100, c='k', zorder=10)\n",
    "display(theta)\n",
    "display(thetas[np.argmin(np.sum(thetas**2, axis=1)), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.lstsq(A, y, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigando nos damos cuenta que `linalg.lstsq` está basado en la función de LAPACK [`dgels`](https://www.math.utah.edu/software/lapack/lapack-d/dgels.html)\n",
    "\n",
    "dgels usa la pseudo inversa izquierda si $N<M$ o la pseudo inversa derecha si $N>M$\n",
    "\n",
    "> Se asume que la mejor solución del sistema infradeterminado es la de **mínima norma euclidiana**\n",
    "\n",
    "## Complejidad, sobreajuste y regularización\n",
    "\n",
    "Un modelo con más parámetros es más flexible pero también más complejo\n",
    "\n",
    "Un exceso de flexibilidad no es bueno. Podría ocurrir que:\n",
    "\n",
    "El modelo se ajuste al ruido y ya no generalice bien ante nuevos datos\n",
    "\n",
    "> Se dice entonces que el modelo se ha **sobreajustado a los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=30)\n",
    "y_clean = np.poly1d([2, -4, 20])(x) # 2*x**2 -4*x -10\n",
    "y = y_clean + 3*np.random.randn(len(x))\n",
    "fig, ax = plt.subplots(figsize=( 4, 4), tight_layout=True)\n",
    "ax.scatter(x, y); \n",
    "ax.plot(x, y_clean, lw=2, alpha=.5)\n",
    "ax.set_xlabel('x'); ax.set_ylabel('y');\n",
    "\n",
    "A = poly_basis(x, N=29)\n",
    "theta = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "ax.plot(x, np.dot(A, theta), 'k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tres maneras de evitar el sobreajuste\n",
    "\n",
    "- Usar modelos de baja complejidad \n",
    "- Escoger la complejidad usando validación cruzada\n",
    "- Usar **regularización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig = plt.figure(figsize=(6, 5), tight_layout=True)\n",
    "x = np.linspace(-5, 6, num=11); \n",
    "x_plot = np.linspace(-5, 6, num=100);\n",
    "theta = [10, -2, -0.3, 0.1]\n",
    "X = poly_basis(x, len(theta))\n",
    "y = np.dot(X, theta)\n",
    "\n",
    "def update(sigma, rseed, N):\n",
    "    np.random.seed(rseed); \n",
    "    Y = np.dot(X, theta) + sigma*np.random.randn(len(x))\n",
    "    X2 = poly_basis(x, N)\n",
    "    theta_hat = np.linalg.lstsq(X2[:10, :], Y[:10], rcond=None)[0]\n",
    "    print(theta, theta_hat)\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax.scatter(x[:10], Y[:10], c='r', s=100, label='train data')\n",
    "    ax.scatter(x[10:], Y[10:], c='g', s=100, label='test data')\n",
    "    ax.vlines(x[:10], np.dot(X2[:10, :], theta_hat), Y[:10], 'r')  \n",
    "    ax.vlines(x[10:], np.dot(X2[10:, :], theta_hat), Y[10:], 'g') \n",
    "    ax.plot(x, y, 'b--', linewidth=4, label='underlying')\n",
    "    ax.plot(x_plot, np.dot(poly_basis(x_plot, N), theta_hat), 'k-', linewidth=4, label='model')\n",
    "    ax.set_ylim([-5, 15]); plt.legend()\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0))\n",
    "    ax.plot(x, np.zeros_like(x), 'k--', alpha=0.5)\n",
    "    ax.scatter(x, Y - np.dot(X2, theta_hat), c='k', s=100); \n",
    "    \n",
    "interact(update, rseed=IntSlider(continuous_update=False), \n",
    "         N=SelectionSlider(options=[1, 2, 3, 4, 5, 7, 10]), \n",
    "         sigma=SelectionSlider(options=[0.1, 1, 2, 5]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación\n",
    "\n",
    "Para escoger la cantidad de parámetros de nuestro modelo o escoger hiper-parámetros como $\\lambda$ podemos usar validación cruzada\n",
    "\n",
    "Antes de ajustar el modelo se particionan los datos en dos conjuntos\n",
    "1. Conjunto de entrenamiento: Datos que se ocupan para ajustar el modelo\n",
    "1. Conjuto de validación: Datos que se ocupan para evaluar el modelo\n",
    "\n",
    "Nos quedamos con el modelo que se desempeña mejor en validación \n",
    "\n",
    "Un modelo sobreajustado tiene buen desempeño en entrenamiento y malo en validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización\n",
    "\n",
    "Consiste en agregar una penalización adicional al problema \n",
    "\n",
    "El ejemplo clásico es pedir que la solución tenga norma mínima\n",
    "\n",
    "$$\n",
    "\\min_x \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2\n",
    "$$\n",
    "\n",
    "En este caso la solución es\n",
    "\n",
    "$$\n",
    "\\hat x = (A^T A + \\lambda I)^{-1} A^T b\n",
    "$$\n",
    "\n",
    "que se conoce como **ridge regression** o **regularización de Tikhonov**\n",
    "\n",
    "$\\lambda$ es un hiper-parámetro del modelo y debe ser escogido por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = np.linspace(-5, 6, num=50); x_plot = np.linspace(-5, 6, num=200); \n",
    "model = np.sin(x)*x + 0.1*x**2\n",
    "print(repr(theta))\n",
    "\n",
    "def update(sigma, rseed, lamb, M):\n",
    "    np.random.seed(rseed); \n",
    "    y = model + sigma*np.random.randn(len(x))\n",
    "    regressor = make_pipeline(PolynomialFeatures(M), Ridge(normalize=True, alpha=lamb))\n",
    "    regressor.fit(x.reshape(-1, 1), y)\n",
    "    ax.cla(); ax.plot(x, model, 'b--', linewidth=4, label='underlying')\n",
    "    ax.plot( x_plot , regressor.predict( x_plot .reshape(-1, 1)), 'k-', linewidth=4, label='model')\n",
    "    ax.scatter(x, y, c='r', s=30, label='data', zorder=100); plt.legend()\n",
    "\n",
    "    \n",
    "interact(update, rseed=IntSlider(continuous_update=False), M=SelectionSlider(options=[1, 2, 3, 5, 10, 20]), \n",
    "         sigma=SelectionSlider(options=[0.1, 1, 2, 5]),\n",
    "         lamb=SelectionSlider(options=[0.0, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1., 100000.]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opciones de más alto nivel para hacer regresión lineal\n",
    "\n",
    "- `scipy.stats.linregress`\n",
    "- [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outliers\n",
    "https://blog.datarobot.com/ordinary-least-squares-in-python\n",
    "    https://mmas.github.io/least-squares-fitting-numpy-scipy\n",
    "        https://docs.scipy.org/doc/scipy-1.3.0/reference/tutorial/linalg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sbu-python-class.github.io/python-science/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
