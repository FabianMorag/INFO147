{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices como transformaciónes lineales\n",
    "\n",
    "Si tenemos una base de datos con atributos continuos podemos representar cada ejemplo como un vector $\\{v_i\\}$, $i=1,2,\\ldots,M$.\n",
    "\n",
    "Asumamos que $v_i \\in \\mathbb{R}^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 4), tight_layout=True)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim([-3, 3]); ax.set_ylim([-3, 3]); \n",
    "v = np.random.randn(10, 2)\n",
    "for v_ in v:\n",
    "    ax.arrow(0, 0, v_[0], v_[1], color='k', **arrow_args)\n",
    "ax.scatter(v[:, 0], v[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos transformar nuestros datos usando una matriz cuadrada $A \\in \\mathbb{R}^{2\\times 2}$, en este caso digamos $A = \\begin{pmatrix} a_1 & a_{2} \\\\ a_{3} & a_4 \\end{pmatrix}$\n",
    "\n",
    "¿Qué ocurre con un vector $v = \\rho \\begin{pmatrix}\\sin(\\theta) \\\\ \\cos(\\theta)\\end{pmatrix}$ cuando modificamos los componentes de la matriz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(6, 4), tight_layout=True)\n",
    "world_axis = np.array([[1, 0], [0, 1]])\n",
    "arrow_args = {'width': 0.05, 'length_includes_head': True}\n",
    "\n",
    "def update_plot(rho, theta, a1, a2, a3, a4):\n",
    "    A = [[a1, a2], [a3, a4]]\n",
    "    new_world_axis = np.dot(A, world_axis)\n",
    "    print(new_world_axis)\n",
    "    for ax_ in ax:\n",
    "        ax_.cla(); ax_.set_aspect('equal')\n",
    "        ax_.set_xlim([-3, 3]); ax_.set_ylim([-3, 3]); \n",
    "    ax[0].set_title(r'$\\vec v$')\n",
    "    ax[1].set_title(r'$A \\vec v$')\n",
    "    for j in range(2):\n",
    "        ax[0].arrow(0, 0, world_axis[0, j], world_axis[1, j], color='gray', alpha=0.5, **arrow_args)\n",
    "        ax[1].arrow(0, 0, new_world_axis[0, j], new_world_axis[1, j], color='gray', alpha=0.5, **arrow_args)\n",
    "    v = rho*np.array([np.sin(theta), np.cos(theta)])\n",
    "    ax[0].arrow(0, 0, v[0], v[1], color='k', **arrow_args)\n",
    "    ax[1].arrow(0, 0, v[0], v[1], color='b', **arrow_args)\n",
    "    v = np.dot(A, v)\n",
    "    ax[1].arrow(0, 0, v[0], v[1], color='k', **arrow_args)\n",
    "    evals, evecs = scipy.linalg.eig(A)\n",
    "\n",
    "widgets.interact(update_plot, \n",
    "                 rho=FloatSlider_nice(min=0.1, max=2, value=1.),\n",
    "                 theta=FloatSlider_nice(min=-np.pi, max=np.pi, value=np.pi/4, step=1e-2),\n",
    "                 a1=FloatSlider_nice(min=-2, max=2, value=1.), \n",
    "                 a2=FloatSlider_nice(min=-2, max=2, value=0.), \n",
    "                 a3=FloatSlider_nice(min=-2, max=2, value=0),\n",
    "                 a4=FloatSlider_nice(min=-2, max=2, value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos una transformación en particular  $A = \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{pmatrix}$ \n",
    "\n",
    "Podemos notar que para algunos ángulos el vector transformado tiene la misma orientación que el original\n",
    "\n",
    "Pruebe por ejemplo $\\theta = \\pm \\pi/4 \\approx \\pm 0.7854$, es decir $v = \\frac{\\rho}{\\sqrt{2}} \\begin{pmatrix}  \\pm 1 \\\\ 1\\end{pmatrix}$\n",
    "\n",
    "> Los vectores que tienen ese ángulo solo son afectados por $A$ en su magnitud \n",
    "\n",
    "Esos vectores se conocen como los **vectores propios** de $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema de los valores/vectores propios\n",
    "\n",
    "Sea una matriz cuadrada $A \\in \\mathbb{R}^{M\\times M}$\n",
    "\n",
    "El siguiente sistema de ecuaciones de $M$ ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "A \\vec v &= \\lambda I \\vec v \\\\\n",
    "(A - \\lambda I) \\vec v &= 0,\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "tiene como resultado $\\lambda$, los valores propios de $A$ y $\\vec v$ los vectores propios de $A$\n",
    "\n",
    "La solución no trivial de este problema se obtiene si $(A - \\lambda I)$ es singular, luego su determinante\n",
    "\n",
    "$$\n",
    "|A - \\lambda I | = 0\n",
    "$$\n",
    "\n",
    "que resulta en un polinomio de grado $M$ cuyas raices son $\\{\\lambda_i\\}$, $i=1,2,\\ldots, M$\n",
    "\n",
    "Una vez determinado $\\lambda_i$ se pueden usar para despejar $\\vec v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Para la matriz $A$ del ejemplo, si igualamos su determinante a cero tenemos\n",
    "\n",
    "$$\n",
    "(1 - \\lambda)^2 - 1/4 = 3/4 - 2\\lambda + \\lambda^2 = 0\n",
    "$$\n",
    "\n",
    "osea $\\lambda_1 = 1.5$ y $\\lambda_2 = 0.5$. Luego para el primer vector propio tenemos un sistema de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "-0.5v_{11} +0.5v_{12} &= 0 \\\\\n",
    "0.5 v_{11} -0.5v_{12} &= 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "osea $v_{11} = v_{12}$ con esto podemos construir un vector normalizado genérico $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "De forma equivalente para $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "### Con computador\n",
    "\n",
    "Podemos usar las funciones de [`scipy.linalg`](https://docs.scipy.org/doc/scipy/reference/linalg.html#eigenvalue-problems) `eig()` o `eigvals()` (y sus variantes) para resolver el sistema de ecuaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "\n",
    "A = np.array([[1., 0.5], [0.5, 1]])\n",
    "evals, evecs = scipy.linalg.eig(A)\n",
    "display(evals, evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descomposicion en valores propios\n",
    "\n",
    "Existen [múltiples sistemas en física](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Applications) que [ocurren naturalmente](https://hubpages.com/education/What-the-Heck-are-Eigenvalues-and-Eigenvectors) como un problema de valores/vectores propios\n",
    "\n",
    "Sin embargo la aplicación más amplia para este elemento matemático es la **descomposición en vectores propios**\n",
    "\n",
    "> Descomponer: Expresar un elemento como una suma de partes de más simples\n",
    "\n",
    "La descomposición que veremos a continuación usa los **vectores propios** como \"las partes simples\"\n",
    "\n",
    "- ¿Cómo encontrar los vectores propios de una base de datos? ¿Qué intepretación tienen?\n",
    "- ¿Cómo descomponemos nuestros datos en función de los vectores propios?\n",
    "- ¿Qué ventaja tiene esta descomposición?\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Es un procedimiento estadístico que busca una **transformación ortogonal** para los datos que logre **maximizar su varianza**\n",
    "\n",
    "Para un conjunto de datos $\\{x_i\\}$ con $i=1,2,\\ldots, M$ y $x_i \\in \\mathbb{R}^D$\n",
    "\n",
    "Podemos escribirlo como una matriz $X \\in \\mathbb{R}^{D\\times M}$\n",
    "\n",
    "Podemos calcular su matriz de correlación como \n",
    "$$\n",
    "C = \\frac{1}{M} (X - \\bar X) ( X - \\bar X)^T\n",
    "$$\n",
    "\n",
    "donde $C \\in \\mathbb{R}^{D\\times D}$ y $\\bar X$ es la media del conjunto\n",
    "\n",
    "Llamemos $U$ a la matriz de proyección. El problema de PCA se puede escribir como\n",
    "\n",
    "$$\n",
    "\\max_U U C U^T \\text{ sujeto a } U^T U = I\n",
    "$$\n",
    "\n",
    "Si usamos multiplicadores de Lagrange para incluir la restricción y luego derivamos e igualamos a cero\n",
    "\n",
    "$$\n",
    "\\frac{d}{dU} U^T C U + \\Lambda(I- U^T U) = CU - \\Lambda U = 0\n",
    "$$\n",
    "\n",
    "donde $\\Lambda = \\lambda I$ y $\\lambda = (\\lambda_1, \\lambda_2, \\ldots, \\lambda_D)$\n",
    "\n",
    "> La transformación de PCA son los vectores propios de $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Breast cancer\n",
    "\n",
    "Mostrar ejes proyectados\n",
    "\n",
    "Mostrar atributos del vp 1 y 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: *Eigen-faces* o Rostros principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "lfw_people = sklearn.datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people['data'].T\n",
    "display(X.shape)\n",
    "fig, ax = plt.subplots(3, 7, figsize=(7, 4))\n",
    "for i, ax_ in enumerate(ax.ravel()):\n",
    "    ax_.axis('off')\n",
    "    ax_.imshow(X[:, i].reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7, 2))\n",
    "ax.axis('off')\n",
    "ax.imshow(np.mean(X, axis=1).reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=1, keepdims=True)\n",
    "X_center = X - X_mean\n",
    "C = np.dot(X_center, X_center.T)\n",
    "display(C.shape)\n",
    "L, U = scipy.linalg.eigh(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(L)[::-1] # Ordenar de L más grande a más pequeño\n",
    "L = L[idx]\n",
    "U = U[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 7, figsize=(7, 4.5), tight_layout=True)\n",
    "for i, ax_ in enumerate(ax.ravel()):\n",
    "    ax_.axis('off')\n",
    "    ax_.set_title(\"{0:0.1e}\".format(L[i]))\n",
    "    ax_.imshow(U[:, i].reshape(50, 37), cmap=plt.cm.Greys_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto calcula los coeficientes de la imagen 0\n",
    "P = np.dot(U.T, X_center[:, 0][:, None])\n",
    "# Esto regenera la imagen cero a partir de sus coeficientes\n",
    "Xhat = X_mean + np.dot(U, P)\n",
    "display(np.allclose(X[:, 0], Xhat[:, 0], rtol=1e-2))\n",
    "# ¿Cuantos coeficientes se necesitan para que X se parezca a Xhat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(5, 2))\n",
    "ax[0].axis('off'); ax[1].axis('off')\n",
    "ax[0].imshow(X[:, 0].reshape(50, 37), cmap=plt.cm.Greys_r)\n",
    "\n",
    "def update_plot(k):\n",
    "    Xhat = X_mean + np.dot(U[:, :k], P[:k])\n",
    "    ax[1].imshow(Xhat.reshape(50, 37), cmap=plt.cm.Greys_r)\n",
    "widgets.interact(update_plot, k=SelSlider_nice(options=[1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 1850]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "TODO\n",
    "\n",
    "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
